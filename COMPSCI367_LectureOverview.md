# COMPSCI 367 Lecture Overview

## Lecture 20
* Bayes' rule
* Bayes' rule with existing evidence
* Condiionally independence
* Huffman coding
* Entropy

## Lecture 21
* Principle of indifference
* Non-informative prior
* Conditional probability distribution
* Belief/Bayesian network
* Local Markov property

## Lecture 22
* Belief network inference problem1
* Chain rule
* Variable elimination algorithm
* Normalisation
* Factors
	* Restriction
	* Multiplication
	* Summing out

## Lecture 25
* Markov chain
* Stationary property
* Stochastic process
* State
* Transitions
* Initial conditions
* Transition probability
* Hidden markov model

## Lecture 26
* Filtering problem
* Forward algorithm

## Lecture 28
* Experience
* Performance
* Tasks
* Architecture of a learning agent
* Supervised learning
* Input features
* Target features
* Training set
* Test set
* Classification
* Classes
* Regression
* Memorisation
* Lazy learning
* Rote learner algorithm
* Nearest neighbour classifier
* Distance function
* val(e, X) notation
* Hamming distance
* Euclidean distance
* Manhatten distance

## vLecture 29
* Voronoi diagram
* Voronoi cell
* Overfitting
* k-Nearest Neighbour classifier
* Order-k Voronoi diagram
* Order-k Voronoi cell
* Case-based reasoning

## Lecture 30
* Rule-based
* Decision tree
* Decision tree classifier
* Possible model
* Hypothesis space
* Decision tree learner
* Ockham's razor
* Top-down induction decision tree (TDIDT)
* Stopping criteron
* Point estimate
* Select feature
* Exhaustive split
* Perfect classification
* Local optimality
* Majority rule
* Median or mean
* Probability distribution

## Lecture 31
* Entropy
* Conditional Entropy
* Information gain
* Maximum information gain
* ID3 algorithm
* Split information
* Gain ratio
* C4.5 algorithm
* Gini index
* Classification and regression trees (CART)

## Lecture 32
* Bayesian classifiers
* Strong independence assumption in navie Bayesian classifiers
* Classification using a naive Bayesian classifier
* Maximum	likelihood estimate
* Bernoulli distribution

## Lecture 34
* Sum of squares error
* Linear regression
* Gradient descent
* Learning rate
* GDLinearLearner algorithm
* Linearly seperable
* Linear classifier
* Weight vector
* Perceptron algorithm

## Lecture 35
* Activation function
* Logistic function

## Lecture 36
* Feedforward neural network
* Input nodes
* Hidden nodes
* Output nodes
* Backpropagation

## Exam topics
Ian
1. Agents
2. Turing Test, Chinese room
4. Knowledge engineering
4. Decision tree
5. Philosophy of Artificial Intelligence

Jiamou
1. Bayesian probability
2. Belief networks, Variable elimination, Sampling
3. Markov chains, Hidden Markov models
4. Instance-based Machine learning
5. Decision tree Machine learning (ID3, C4.5)
6. Bayesian learning (Naive Bayesian classifiers)
7. Linear gradient descent
8. Perceptron, Feedforward neural networks
